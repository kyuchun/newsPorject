"""
ë‰´ìŠ¤ ê°ì„± ë¶„ì„ & ìš”ì•½ â€” FastAPI ë°±ì—”ë“œ v1.4
- í•œêµ­ì–´ ê°ì„± ì‚¬ì „ + Google ë²ˆì—­ í´ë°±
"""

import ssl
import os
from contextlib import asynccontextmanager

import nltk
import requests as http_requests
import urllib3
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse
from pydantic import BaseModel
from textblob import TextBlob
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

# â”€â”€ SSL ì „ì—­ ìš°íšŒ (ê¸°ì—… ë„¤íŠ¸ì›Œí¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
os.environ["CURL_CA_BUNDLE"] = ""
os.environ["REQUESTS_CA_BUNDLE"] = ""

_orig_request = http_requests.Session.request
def _patched_request(self, *args, **kwargs):
    kwargs["verify"] = False
    return _orig_request(self, *args, **kwargs)
http_requests.Session.request = _patched_request

from deep_translator import GoogleTranslator


# â”€â”€ NLTK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _ensure_nltk_data():
    for res in ("punkt", "punkt_tab"):
        try:
            nltk.data.find(f"tokenizers/{res}")
        except LookupError:
            try:
                _c = ssl._create_default_https_context
                ssl._create_default_https_context = ssl._create_unverified_context
                nltk.download(res, quiet=True)
            finally:
                ssl._create_default_https_context = _c


@asynccontextmanager
async def lifespan(app: FastAPI):
    _ensure_nltk_data()
    yield


app = FastAPI(
    title="ë‰´ìŠ¤ ê°ì„± ë¶„ì„ & ìš”ì•½ API",
    description="v1.4 â€” í•œêµ­ì–´ ê°ì„± ì‚¬ì „ + ë²ˆì—­ í´ë°±",
    version="1.4.0",
    lifespan=lifespan,
)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

SUMY_LANG_MAP = {"en": "english", "ko": "english", "ja": "japanese", "de": "german", "fr": "french", "es": "spanish"}


# â”€â”€ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SentimentResult(BaseModel):
    label: str
    polarity: float
    analyzed_text: str
    method: str


class ArticleResult(BaseModel):
    title: str
    description: str
    content: str
    url: str
    source: str
    published: str
    image_url: str | None
    sentiment: SentimentResult
    summary: str


class NewsResponse(BaseModel):
    total: int
    sentiment_summary: dict[str, int]
    articles: list[ArticleResult]


class SentimentRequest(BaseModel):
    text: str
    language: str = "auto"


class SummarizeRequest(BaseModel):
    text: str
    sentence_count: int = 3
    language: str = "en"


# â”€â”€ ê°ì„± ì‚¬ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EN_POSITIVE = {
    "good", "great", "best", "win", "success", "grow", "surge", "boost",
    "rise", "gain", "profit", "record", "breakthrough", "improve", "love",
    "happy", "excellent", "strong", "launch", "innovation", "support",
    "recover", "positive", "hope", "top", "high", "lead", "award",
    "joy", "wonderful", "fantastic", "amazing", "delight", "pleasure",
    "beautiful", "brilliant", "awesome", "celebrate", "glad", "cheerful",
    "excited", "thrilled", "optimistic", "proud", "grateful", "fortunate",
    "happiness", "satisfaction", "impressive", "remarkable", "outstanding",
}

EN_NEGATIVE = {
    "bad", "worst", "fail", "loss", "crash", "fall", "drop", "kill",
    "death", "war", "attack", "crisis", "fear", "threat", "decline",
    "down", "cut", "fire", "fraud", "scandal", "risk", "danger",
    "collapse", "destroy", "bomb", "terror", "recession", "layoff",
    "victim", "suffer", "wrong", "debt", "bankrupt", "negative",
    "sad", "terrible", "horrible", "awful", "pain", "grief", "angry",
    "hate", "misery", "sorrow", "tragedy", "disaster", "despair",
    "worried", "disappointed", "frustrated", "alarming", "devastating",
    "sadness", "anxiety", "depression", "loneliness", "regret",
}

# í•œêµ­ì–´ ê°ì„± ì‚¬ì „ â€” ë¶€ë¶„ ë§¤ì¹­(í¬í•¨ ì—¬ë¶€)ìœ¼ë¡œ ê²€ì‚¬
KO_POSITIVE = [
    "ê¸°ì¨", "ê¸°ì˜", "í–‰ë³µ", "ì‚¬ë‘", "ì¢‹ë‹¤", "ì¢‹ì€", "ì¢‹ì•„", "í›Œë¥­", "ë©‹ì§„", "ë©‹ì§€",
    "ìµœê³ ", "ì„±ê³µ", "ê°ì‚¬", "ì¦ê±°", "ì¦ê²", "ì¶•í•˜", "ìŠ¹ë¦¬", "í¬ë§", "ì‘ì›", "ëŒ€ë°•",
    "ìš°ìˆ˜", "íƒì›”", "ê¸ì •", "ë§Œì¡±", "í™˜ì˜", "ê°ë™", "ë¿Œë“¯", "ìë‘", "ì‹ ë‚˜", "ì‹ ë‚œ",
    "ì›ƒìŒ", "í™˜í˜¸", "ì°¬ì‚¬", "ë³´ëŒ", "ì„¤ë ˆ", "ê¸°ëŒ€", "ì˜í–ˆ", "ì˜í•œ", "ìµœì„ ", "ë°œì „",
    "ìƒìŠ¹", "í˜¸ì¡°", "í‘ì", "ì´ìµ", "ìˆ˜ìµ", "ê²½ì‚¬", "ë³µ", "ê±´ê°•", "í‰í™”", "í™”í•©",
]

KO_NEGATIVE = [
    "ìŠ¬í””", "ìŠ¬í”„", "ë¶„ë…¸", "ì‹¤íŒ¨", "ë‚˜ì˜", "ë‚˜ìœ", "ìµœì•…", "ìœ„ê¸°", "ì „ìŸ", "ì‚¬ë§",
    "ì£½ìŒ", "ì£½ì—ˆ", "ê³µí¬", "ë¶ˆì•ˆ", "ê±±ì •", "ê³ í†µ", "ì ˆë§", "ë¹„ê·¹", "ì¬ë‚œ", "íŒŒê´´",
    "ì†ì‹¤", "í•˜ë½", "í­ë½", "ì¹¨ì²´", "ë¶€ì •", "í˜ì˜¤", "ì¦ì˜¤", "ê´´ë¡œ", "ëˆˆë¬¼", "í›„íšŒ",
    "ì‹¤ë§", "íŒ¨ë°°", "ì¢Œì ˆ", "ë‘ë ¤", "ìš°ìš¸", "ì™¸ë¡œ", "ê³ ë…", "ì›ë§", "ì ì", "íŒŒì‚°",
    "í•´ê³ ", "ì‚¬ê³ ", "í”¼í•´", "ìœ„í—˜", "í­ë ¥", "ë²”ì£„", "í…ŒëŸ¬", "ë¹š", "ë¶€ì±„", "íƒ„í•µ",
]


def _is_korean(text: str) -> bool:
    for ch in text:
        if "\uac00" <= ch <= "\ud7a3" or "\u3131" <= ch <= "\u3163":
            return True
    return False


def _ko_sentiment_score(text: str) -> tuple[float, int, int]:
    """í•œêµ­ì–´ ê°ì„± ì‚¬ì „ìœ¼ë¡œ ì§ì ‘ ì ìˆ˜ ê³„ì‚° (ë¶€ë¶„ ë§¤ì¹­)."""
    pos = sum(1 for w in KO_POSITIVE if w in text)
    neg = sum(1 for w in KO_NEGATIVE if w in text)
    score = (pos - neg) * 0.25
    return max(-1.0, min(1.0, score)), pos, neg


def _translate_to_english(text: str) -> tuple[str, bool]:
    """Google ë²ˆì—­ ì‹œë„. ë°˜í™˜: (ê²°ê³¼, ì„±ê³µì—¬ë¶€)"""
    try:
        result = GoogleTranslator(source="auto", target="en").translate(text)
        if result and result.strip() and result.strip().lower() != text.strip().lower():
            return result.strip(), True
        return text, False
    except Exception as e:
        print(f"[ë²ˆì—­ ì‹¤íŒ¨] {e}")
        return text, False


def _is_valid_text(text: str) -> bool:
    if not text or not text.strip():
        return False
    return not any(m in text for m in ["[removed]", "[Removed]", "(ì œëª© ì—†ìŒ)"])


def _analyze_sentiment(text: str, language: str = "auto") -> SentimentResult:
    """
    ê°ì„± ë¶„ì„ â€” 3ë‹¨ê³„ ì „ëµ:
      1) í•œêµ­ì–´ ê°ì„± ì‚¬ì „ (ì¦‰ì‹œ)
      2) ì˜ì–´ ë²ˆì—­ + TextBlob + ì˜ì–´ í‚¤ì›Œë“œ ì‚¬ì „
      3) ë‘ ê²°ê³¼ ì¢…í•©
    """
    if not _is_valid_text(text):
        return SentimentResult(label="Neutral ğŸ˜", polarity=0.0, analyzed_text="(ë¶„ì„ ë¶ˆê°€)", method="none")

    is_ko = _is_korean(text)
    method_used = ""

    # â”€â”€ 1ë‹¨ê³„: í•œêµ­ì–´ ì‚¬ì „ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ko_score, ko_pos, ko_neg = 0.0, 0, 0
    if is_ko:
        ko_score, ko_pos, ko_neg = _ko_sentiment_score(text)
        print(f"[1-KOì‚¬ì „] '{text}' â†’ ê¸ì •={ko_pos}, ë¶€ì •={ko_neg}, ì ìˆ˜={ko_score:.2f}")

    # â”€â”€ 2ë‹¨ê³„: ì˜ì–´ ë²ˆì—­ + TextBlob â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    en_text = text
    translated = False
    tb_polarity = 0.0
    en_boost = 0.0

    if is_ko or language not in ("en",):
        en_text, translated = _translate_to_english(text)
        print(f"[2-ë²ˆì—­] '{text}' â†’ '{en_text}' (ì„±ê³µ={translated})")

    if translated or not is_ko:
        blob = TextBlob(en_text)
        tb_polarity = blob.sentiment.polarity
        words = set(en_text.lower().split())
        en_pos = len(words & EN_POSITIVE)
        en_neg = len(words & EN_NEGATIVE)
        en_boost = (en_pos - en_neg) * 0.15
        print(f"[2-TextBlob] pol={tb_polarity:.4f}, EN(+{en_pos}/-{en_neg}), boost={en_boost:.2f}")

    # â”€â”€ 3ë‹¨ê³„: ì¢…í•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_ko and translated:
        final = ko_score * 0.4 + tb_polarity * 0.3 + en_boost * 0.3
        method_used = f"KOì‚¬ì „({ko_score:.2f})+ë²ˆì—­TextBlob({tb_polarity:.2f})+ENí‚¤ì›Œë“œ({en_boost:.2f})"
    elif is_ko and not translated:
        final = ko_score
        method_used = f"KOì‚¬ì „ only({ko_score:.2f})"
    else:
        final = tb_polarity * 0.6 + en_boost * 0.4
        method_used = f"TextBlob({tb_polarity:.2f})+ENí‚¤ì›Œë“œ({en_boost:.2f})"

    final = max(-1.0, min(1.0, final))
    print(f"[ìµœì¢…] '{text}' â†’ {final:.4f} ({method_used})")

    if final > 0.03:
        label = "Positive ğŸ˜Š"
    elif final < -0.03:
        label = "Negative ğŸ˜Ÿ"
    else:
        label = "Neutral ğŸ˜"

    display_text = en_text if translated else text
    return SentimentResult(label=label, polarity=round(final, 4), analyzed_text=display_text[:100], method=method_used)


def _summarize_text(text: str, sentence_count: int = 3, lang: str = "en") -> str:
    if not text or len(text.split()) < 10:
        return text or "(ë³¸ë¬¸ ì—†ìŒ)"
    sumy_lang = SUMY_LANG_MAP.get(lang, "english")
    try:
        parser = PlaintextParser.from_string(text, Tokenizer(sumy_lang))
        stemmer = Stemmer(sumy_lang)
        summarizer = LsaSummarizer(stemmer)
        summarizer.stop_words = get_stop_words(sumy_lang)
        summary = summarizer(parser.document, sentence_count)
        return " ".join(str(s) for s in summary) or "(ìš”ì•½ ìƒì„± ì‹¤íŒ¨)"
    except Exception as e:
        return f"(ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e})"


def _fetch_news(api_key: str, query: str, language: str, page_size: int) -> list[dict]:
    url = "https://newsapi.org/v2/everything"
    params = {"q": query, "language": language, "pageSize": page_size, "sortBy": "publishedAt", "apiKey": api_key}
    resp = http_requests.get(url, params=params, timeout=15, verify=False)
    resp.raise_for_status()
    data = resp.json()
    if data.get("status") != "ok":
        raise ValueError(data.get("message", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"))
    return data.get("articles", [])


# â”€â”€ ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/", include_in_schema=False)
def root():
    return RedirectResponse(url="/docs")

@app.get("/health")
def health():
    return {"status": "ok", "version": "1.4.0"}


@app.get("/news", response_model=NewsResponse)
def get_news(
    api_key: str = Query(..., description="NewsAPI í‚¤"),
    query: str = Query("technology", description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    language: str = Query("en", description="ì–¸ì–´ ì½”ë“œ (en, ko, ja, de, fr, es)"),
    page_size: int = Query(5, ge=1, le=20, description="ê¸°ì‚¬ ìˆ˜"),
    summary_sentences: int = Query(3, ge=1, le=10, description="ìš”ì•½ ë¬¸ì¥ ìˆ˜"),
):
    """ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê³ , ê°ì„± ë¶„ì„ ë° ë³¸ë¬¸ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        raw = _fetch_news(api_key, query, language, page_size)
    except Exception as e:
        raise HTTPException(502, detail=f"ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    if not raw:
        return NewsResponse(total=0, sentiment_summary={}, articles=[])

    counts = {"Positive ğŸ˜Š": 0, "Neutral ğŸ˜": 0, "Negative ğŸ˜Ÿ": 0}
    results = []

    for a in raw:
        title = a.get("title") or "(ì œëª© ì—†ìŒ)"
        desc = a.get("description") or ""
        content = a.get("content") or desc
        analysis = title + (f". {desc}" if desc and _is_valid_text(desc) else "")

        sent = _analyze_sentiment(analysis, language)
        counts[sent.label] = counts.get(sent.label, 0) + 1

        results.append(ArticleResult(
            title=title, description=desc, content=content,
            url=a.get("url", "#"), source=a.get("source", {}).get("name", ""),
            published=(a.get("publishedAt") or "")[:10], image_url=a.get("urlToImage"),
            sentiment=sent, summary=_summarize_text(content, summary_sentences, language),
        ))

    return NewsResponse(total=len(results), sentiment_summary=counts, articles=results)


@app.post("/sentiment", response_model=SentimentResult)
def sentiment(body: SentimentRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not body.text.strip():
        raise HTTPException(400, "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return _analyze_sentiment(body.text, body.language)


@app.post("/summarize")
def summarize(body: SummarizeRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    if not body.text.strip():
        raise HTTPException(400, "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return {"summary": _summarize_text(body.text, body.sentence_count, body.language)}
